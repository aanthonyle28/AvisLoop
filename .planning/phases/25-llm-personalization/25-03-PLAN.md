---
phase: 25-llm-personalization
plan: 03
type: execute
wave: 2
depends_on: ["25-01", "25-02"]
files_modified:
  - lib/ai/personalize.ts
  - lib/ai/fallback.ts
  - lib/ai/rate-limit.ts
autonomous: true

must_haves:
  truths:
    - "personalizeMessage() returns personalized text or falls back to template"
    - "LLM failures never block message sending"
    - "Rate limiting enforces 100 LLM calls/hour per business"
    - "Retry logic uses exponential backoff with jitter"
  artifacts:
    - path: "lib/ai/personalize.ts"
      provides: "Core personalization function with structured output"
      exports: ["personalizeMessage", "PersonalizeResult"]
    - path: "lib/ai/fallback.ts"
      provides: "Fallback chain logic with retry and error classification"
      exports: ["personalizeWithFallback", "FallbackReason"]
    - path: "lib/ai/rate-limit.ts"
      provides: "LLM rate limiting per business"
      exports: ["checkLLMRateLimit", "LLMRateLimitError"]
  key_links:
    - from: "lib/ai/personalize.ts"
      to: "lib/ai/client.ts"
      via: "getModel import"
      pattern: "import.*getModel.*from.*client"
    - from: "lib/ai/fallback.ts"
      to: "lib/ai/personalize.ts"
      via: "personalizeMessage import"
      pattern: "import.*personalizeMessage.*from.*personalize"
    - from: "lib/ai/rate-limit.ts"
      to: "@upstash/ratelimit"
      via: "Ratelimit class"
      pattern: "Ratelimit\\.slidingWindow"
---

<objective>
Create the core personalization function with fallback chain, retry logic, and rate limiting.

Purpose: This is the heart of LLM personalization. The personalizeMessage() function calls the LLM with structured output, validates the result, and gracefully falls back to raw template on any failure. Rate limiting prevents runaway costs.

Output: Complete lib/ai/ module ready for integration with campaign touch processor and preview components.
</objective>

<execution_context>
@C:\Users\aanth\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\aanth\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/25-llm-personalization/25-CONTEXT.md
@.planning/phases/25-llm-personalization/25-RESEARCH.md
@lib/rate-limit.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create LLM rate limiting for personalization</name>
  <files>lib/ai/rate-limit.ts</files>
  <action>
Create `lib/ai/rate-limit.ts` extending existing rate limit patterns from lib/rate-limit.ts:

```typescript
import { Ratelimit } from '@upstash/ratelimit'
import { Redis } from '@upstash/redis'

// LLM rate limit per business: 100 calls/hour
// Matches CONTEXT.md "Rate limiting per business: 100 LLM calls/hour"
const LLM_RATE_LIMIT = 100
const LLM_RATE_WINDOW = '1 h'

// Lazy-initialized rate limiter (only if Redis configured)
let llmRateLimiter: Ratelimit | null = null

function getLLMRateLimiter(): Ratelimit | null {
  if (!process.env.UPSTASH_REDIS_REST_URL || !process.env.UPSTASH_REDIS_REST_TOKEN) {
    return null  // Dev mode bypass
  }

  if (!llmRateLimiter) {
    const redis = new Redis({
      url: process.env.UPSTASH_REDIS_REST_URL,
      token: process.env.UPSTASH_REDIS_REST_TOKEN,
    })

    llmRateLimiter = new Ratelimit({
      redis,
      limiter: Ratelimit.slidingWindow(LLM_RATE_LIMIT, LLM_RATE_WINDOW),
      analytics: true,
      prefix: 'ratelimit:llm',
    })
  }

  return llmRateLimiter
}

export class LLMRateLimitError extends Error {
  constructor(
    public readonly businessId: string,
    public readonly remaining: number,
    public readonly resetAt: Date
  ) {
    super(`LLM rate limit exceeded for business ${businessId}`)
    this.name = 'LLMRateLimitError'
  }
}

/**
 * Check LLM rate limit for a business.
 * Returns remaining count if within limit.
 * Throws LLMRateLimitError if exceeded.
 *
 * @throws LLMRateLimitError if rate limit exceeded
 */
export async function checkLLMRateLimit(businessId: string): Promise<{
  remaining: number
  resetAt: Date
}> {
  const limiter = getLLMRateLimiter()

  if (!limiter) {
    // No rate limiting configured (dev mode)
    return { remaining: 999, resetAt: new Date() }
  }

  const { success, remaining, reset } = await limiter.limit(businessId)
  const resetAt = new Date(reset)

  if (!success) {
    throw new LLMRateLimitError(businessId, remaining, resetAt)
  }

  return { remaining, resetAt }
}

/**
 * Get current LLM usage for a business (for analytics/UI).
 * Does not decrement counter.
 */
export async function getLLMUsage(businessId: string): Promise<{
  used: number
  limit: number
  remaining: number
  resetAt: Date
} | null> {
  const limiter = getLLMRateLimiter()

  if (!limiter) {
    return null  // Rate limiting not configured
  }

  // Use getRemaining to check without consuming
  const { remaining, reset } = await limiter.getRemaining(businessId)

  return {
    used: LLM_RATE_LIMIT - remaining,
    limit: LLM_RATE_LIMIT,
    remaining,
    resetAt: new Date(reset),
  }
}
```

Design decisions:
- 100 calls/hour per business (from CONTEXT.md)
- Lazy initialization (same pattern as existing rate-limit.ts)
- Custom error class for type-safe handling
- getLLMUsage for analytics without decrementing counter
  </action>
  <verify>
TypeScript compiles: `pnpm typecheck` passes.
File exports checkLLMRateLimit, getLLMUsage, LLMRateLimitError.
  </verify>
  <done>LLM rate limiting enforces 100 calls/hour per business</done>
</task>

<task type="auto">
  <name>Task 2: Create core personalization function</name>
  <files>lib/ai/personalize.ts</files>
  <action>
Create `lib/ai/personalize.ts` with the core LLM personalization logic:

```typescript
import { generateObject } from 'ai'
import { getModel } from './client'
import {
  EMAIL_SYSTEM_PROMPT,
  SMS_SYSTEM_PROMPT,
  buildPersonalizationPrompt,
  type PersonalizationContext
} from './prompts'
import {
  PersonalizedEmailSchema,
  PersonalizedSmsSchema,
  type PersonalizedEmail,
  type PersonalizedSms
} from './schemas'
import { sanitizeAllInputs, validateOutput } from './validation'

export type PersonalizeResult = {
  message: string
  subject?: string  // Only for email
  personalized: boolean
  model?: string
}

/**
 * Personalize a message using LLM.
 * Returns structured output validated against Zod schema.
 *
 * This is the raw LLM call - no fallback handling here.
 * Use personalizeWithFallback() for production code.
 *
 * @throws Error on LLM failure, validation failure, or timeout
 */
export async function personalizeMessage(
  ctx: PersonalizationContext
): Promise<PersonalizeResult> {
  // Sanitize all user-controlled inputs
  const sanitized = sanitizeAllInputs({
    customerName: ctx.customerName,
    businessName: ctx.businessName,
    serviceType: ctx.serviceType,
    technicianName: ctx.technicianName,
  })

  // Build context with sanitized inputs
  const sanitizedCtx: PersonalizationContext = {
    ...ctx,
    ...sanitized,
  }

  // Select system prompt based on channel
  const systemPrompt = ctx.channel === 'email'
    ? EMAIL_SYSTEM_PROMPT
    : SMS_SYSTEM_PROMPT

  // Select schema based on channel
  const schema = ctx.channel === 'email'
    ? PersonalizedEmailSchema
    : PersonalizedSmsSchema

  // Build user prompt
  const userPrompt = buildPersonalizationPrompt(sanitizedCtx)

  // Call LLM with structured output
  const { object, finishReason } = await generateObject({
    model: getModel(),
    schema,
    system: systemPrompt,
    prompt: userPrompt,
    maxRetries: 1,  // AI SDK internal retry (schema correction)
  })

  // Check finish reason
  if (finishReason === 'length') {
    throw new Error('LLM output truncated (length limit)')
  }

  // Extract message based on channel
  if (ctx.channel === 'email') {
    const email = object as PersonalizedEmail

    // Validate output
    const validation = validateOutput(email.body, {
      reviewLink: ctx.reviewLink,
      businessName: ctx.businessName,
      templateLength: ctx.template.length,
      channel: 'email',
    })

    if (!validation.valid) {
      throw new Error(`Output validation failed: ${validation.reason}`)
    }

    return {
      subject: email.subject,
      message: email.body,
      personalized: true,
      model: 'gpt-4o-mini',
    }
  } else {
    const sms = object as PersonalizedSms

    // Validate output
    const validation = validateOutput(sms.body, {
      reviewLink: ctx.reviewLink,
      businessName: ctx.businessName,
      templateLength: ctx.template.length,
      channel: 'sms',
    })

    if (!validation.valid) {
      throw new Error(`Output validation failed: ${validation.reason}`)
    }

    return {
      message: sms.body,
      personalized: true,
      model: 'gpt-4o-mini',
    }
  }
}
```

Key decisions:
- Structured output via generateObject + Zod schema
- Input sanitization before prompt construction
- Output validation after LLM response
- No fallback in this function - handled by wrapper
- Throws on any failure (retry/fallback handled externally)
  </action>
  <verify>
TypeScript compiles: `pnpm typecheck` passes.
File exports personalizeMessage, PersonalizeResult.
  </verify>
  <done>Core personalizeMessage() function calls LLM with structured output and validation</done>
</task>

<task type="auto">
  <name>Task 3: Create fallback chain with retry logic</name>
  <files>lib/ai/fallback.ts</files>
  <action>
Create `lib/ai/fallback.ts` with production-ready fallback handling:

```typescript
import { backOff } from 'exponential-backoff'
import { personalizeMessage, type PersonalizeResult } from './personalize'
import { checkLLMRateLimit, LLMRateLimitError } from './rate-limit'
import type { PersonalizationContext } from './prompts'

export type FallbackReason =
  | 'rate_limited'
  | 'timeout'
  | 'validation_failed'
  | 'api_error'
  | 'retry_exhausted'
  | 'missing_critical_field'

export type PersonalizeWithFallbackResult = {
  message: string
  subject?: string
  personalized: boolean
  fallbackReason?: FallbackReason
  model?: string
}

// Timeout for LLM call (3s from CONTEXT.md)
const LLM_TIMEOUT_MS = 3000

/**
 * Personalize with full fallback chain.
 * NEVER throws - always returns a result (personalized or template fallback).
 *
 * Flow:
 * 1. Check rate limit (throws if exceeded, caught and falls back)
 * 2. Try LLM personalization with timeout + retry
 * 3. Fall back to raw template on any failure
 *
 * This is the function to use in production code.
 */
export async function personalizeWithFallback(
  ctx: PersonalizationContext & { businessId: string }
): Promise<PersonalizeWithFallbackResult> {
  // Critical pre-check: Can't personalize without these
  if (!ctx.reviewLink) {
    return {
      message: ctx.template,
      personalized: false,
      fallbackReason: 'missing_critical_field',
    }
  }

  if (!ctx.customerName || !ctx.businessName) {
    return {
      message: ctx.template,
      personalized: false,
      fallbackReason: 'missing_critical_field',
    }
  }

  // Check rate limit
  try {
    await checkLLMRateLimit(ctx.businessId)
  } catch (error) {
    if (error instanceof LLMRateLimitError) {
      console.log(`LLM rate limit exceeded for business ${ctx.businessId}`)
      return {
        message: ctx.template,
        personalized: false,
        fallbackReason: 'rate_limited',
      }
    }
    // Unexpected error - fall back
    console.warn('Unexpected rate limit error:', error)
    return {
      message: ctx.template,
      personalized: false,
      fallbackReason: 'api_error',
    }
  }

  // Try personalization with retry + timeout
  try {
    const result = await backOff(
      () => withTimeout(personalizeMessage(ctx), LLM_TIMEOUT_MS),
      {
        numOfAttempts: 2,  // Max 2 attempts (initial + 1 retry)
        startingDelay: 500,  // 500ms initial delay
        timeMultiple: 2,  // 500ms, 1000ms
        jitter: 'full',  // Full jitter to prevent thundering herd
        retry: (error) => {
          // Only retry on transient errors
          const message = error.message || ''

          // Retry on timeout
          if (message.includes('timeout') || message.includes('Timeout')) {
            return true
          }

          // Retry on rate limit from provider (429)
          if (message.includes('429') || message.includes('rate limit')) {
            return true
          }

          // Retry on server errors (5xx)
          if (message.includes('500') || message.includes('502') ||
              message.includes('503') || message.includes('504')) {
            return true
          }

          // Don't retry validation failures or 4xx errors
          return false
        },
      }
    )

    return result
  } catch (error) {
    // All retries exhausted or non-retryable error
    const errorMessage = error instanceof Error ? error.message : 'Unknown error'

    console.warn('LLM personalization failed, using template fallback:', {
      error: errorMessage,
      businessId: ctx.businessId,
      channel: ctx.channel,
    })

    // Classify fallback reason
    let fallbackReason: FallbackReason = 'api_error'

    if (errorMessage.includes('timeout') || errorMessage.includes('Timeout')) {
      fallbackReason = 'timeout'
    } else if (errorMessage.includes('validation')) {
      fallbackReason = 'validation_failed'
    } else if (errorMessage.includes('retry')) {
      fallbackReason = 'retry_exhausted'
    }

    return {
      message: ctx.template,
      personalized: false,
      fallbackReason,
    }
  }
}

/**
 * Wrap a promise with a timeout.
 */
function withTimeout<T>(promise: Promise<T>, ms: number): Promise<T> {
  return new Promise((resolve, reject) => {
    const timer = setTimeout(() => {
      reject(new Error(`Operation timed out after ${ms}ms`))
    }, ms)

    promise
      .then((result) => {
        clearTimeout(timer)
        resolve(result)
      })
      .catch((error) => {
        clearTimeout(timer)
        reject(error)
      })
  })
}

/**
 * Batch personalization for preview (multiple messages).
 * Runs in parallel with concurrency limit.
 */
export async function personalizePreviewBatch(
  contexts: Array<PersonalizationContext & { businessId: string }>,
  concurrency: number = 3
): Promise<PersonalizeWithFallbackResult[]> {
  const results: PersonalizeWithFallbackResult[] = []

  // Process in chunks for concurrency control
  for (let i = 0; i < contexts.length; i += concurrency) {
    const chunk = contexts.slice(i, i + concurrency)
    const chunkResults = await Promise.all(
      chunk.map(ctx => personalizeWithFallback(ctx))
    )
    results.push(...chunkResults)
  }

  return results
}
```

Design decisions from CONTEXT.md and RESEARCH.md:
- 3s timeout (from CONTEXT.md auto-fallback triggers)
- 2 attempts max (initial + 1 retry)
- Exponential backoff with jitter
- Never throws - always returns template on failure
- Rate limit checked before LLM call
- Batch function for preview with concurrency control
  </action>
  <verify>
TypeScript compiles: `pnpm typecheck` passes.
File exports personalizeWithFallback, personalizePreviewBatch, FallbackReason.
  </verify>
  <done>Fallback chain provides graceful degradation with retry logic - LLM failures never block sends</done>
</task>

</tasks>

<verification>
All verification commands should pass:

```bash
pnpm typecheck
pnpm lint
```

Check lib/ai/ module is complete:
- lib/ai/client.ts - AI SDK provider
- lib/ai/prompts.ts - System prompts
- lib/ai/schemas.ts - Zod schemas
- lib/ai/validation.ts - Input/output validation
- lib/ai/personalize.ts - Core LLM function
- lib/ai/fallback.ts - Fallback chain
- lib/ai/rate-limit.ts - Rate limiting
</verification>

<success_criteria>
- personalizeWithFallback() returns personalized message or falls back to template
- Rate limiting enforces 100 LLM calls/hour per business
- Retry logic uses exponential backoff with jitter (2 attempts, 500ms/1000ms delays)
- 3-second timeout triggers fallback
- Validation failures trigger immediate fallback (no retry)
- All code typechecks and lints clean
</success_criteria>

<output>
After completion, create `.planning/phases/25-llm-personalization/25-03-SUMMARY.md`
</output>
