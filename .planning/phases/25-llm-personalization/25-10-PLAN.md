---
phase: 25-llm-personalization
plan: 10
type: execute
wave: 1
depends_on: []
files_modified:
  - lib/ai/client.ts
  - lib/ai/personalize.ts
  - lib/ai/fallback.ts
autonomous: true
gap_closure: true
user_setup:
  - service: google-ai
    why: "Gemini Flash is the primary model (70% of calls)"
    env_vars:
      - name: GOOGLE_GENERATIVE_AI_API_KEY
        source: "Google AI Studio -> Get API Key (https://aistudio.google.com/apikey)"
  - service: openrouter
    why: "DeepSeek V3 accessed via OpenRouter (OpenAI-compatible API)"
    env_vars:
      - name: OPENROUTER_API_KEY
        source: "OpenRouter Dashboard -> Keys (https://openrouter.ai/keys)"

must_haves:
  truths:
    - "Three model providers configured: Google (Gemini Flash), OpenAI (GPT-4o-mini), OpenRouter (DeepSeek V3)"
    - "Model routing selects Gemini Flash for bulk/simple, GPT-4o-mini for quality tasks, DeepSeek for edge cases"
    - "Fallback chain tries secondary model before falling back to raw template"
    - "personalizeMessage() uses the routed model, not hardcoded GPT-4o-mini"
  artifacts:
    - path: "lib/ai/client.ts"
      provides: "Multi-provider initialization and model routing"
      exports: ["getModelForTask", "ModelTask", "MODEL_COSTS"]
    - path: "lib/ai/personalize.ts"
      provides: "Uses routed model from client.ts"
      contains: "getModelForTask"
    - path: "lib/ai/fallback.ts"
      provides: "Secondary model fallback before raw template"
      contains: "getModelForTask"
  key_links:
    - from: "lib/ai/personalize.ts"
      to: "lib/ai/client.ts"
      via: "getModelForTask import"
      pattern: "getModelForTask"
    - from: "lib/ai/fallback.ts"
      to: "lib/ai/personalize.ts"
      via: "secondary model retry on validation failure"
      pattern: "secondary"
---

<objective>
Replace single-model (GPT-4o-mini only) setup with multi-model routing: Gemini Flash (70% - bulk/simple), GPT-4o-mini (25% - quality tasks), DeepSeek V3 via OpenRouter (5% - edge cases). Add secondary model fallback in the fallback chain (primary model -> secondary model -> raw template).

Purpose: Close SC #1 gap. Original criteria specified Claude Haiku fallback; user updated to multi-model routing strategy. Also adds cost constants per model for use by cost tracking (Plan 25-11).

Output: Multi-provider client.ts with routing logic, updated personalize.ts using routed model, updated fallback.ts with secondary model retry.
</objective>

<execution_context>
@C:\Users\aanth\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\aanth\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md
@lib/ai/client.ts
@lib/ai/personalize.ts
@lib/ai/fallback.ts
@lib/ai/prompts.ts
@lib/ai/schemas.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install AI SDK providers and update client.ts with multi-model routing</name>
  <files>
    lib/ai/client.ts
  </files>
  <action>
**1. Install @ai-sdk/google package:**

```bash
pnpm add @ai-sdk/google
```

Note: DeepSeek V3 is accessed via OpenRouter, which uses an OpenAI-compatible API. We'll use `@ai-sdk/openai` with a custom `baseURL` for OpenRouter. No additional package needed.

**2. Rewrite `lib/ai/client.ts` with multi-provider setup:**

```typescript
import { createOpenAI } from '@ai-sdk/openai'
import { createGoogleGenerativeAI } from '@ai-sdk/google'

// ============================================================
// Provider Initialization (server-only, keys from env)
// ============================================================

// OpenAI provider (GPT-4o-mini) - 25% of calls
const openai = createOpenAI({
  apiKey: process.env.OPENAI_API_KEY,
})

// Google AI provider (Gemini Flash) - 70% of calls
const google = createGoogleGenerativeAI({
  apiKey: process.env.GOOGLE_GENERATIVE_AI_API_KEY,
})

// OpenRouter provider for DeepSeek V3 - 5% of calls
// Uses OpenAI-compatible API with custom baseURL
const openrouter = createOpenAI({
  apiKey: process.env.OPENROUTER_API_KEY,
  baseURL: 'https://openrouter.ai/api/v1',
})

// ============================================================
// Model Constants
// ============================================================

export const MODELS = {
  GEMINI_FLASH: 'gemini-2.0-flash',
  GPT_4O_MINI: 'gpt-4o-mini',
  DEEPSEEK_V3: 'deepseek/deepseek-chat-v3-0324',
} as const

export type ModelId = typeof MODELS[keyof typeof MODELS]

// Cost per 1M tokens (USD) for each model - used by cost tracking (25-11)
export const MODEL_COSTS: Record<ModelId, { input: number; output: number }> = {
  [MODELS.GEMINI_FLASH]: { input: 0.10, output: 0.40 },
  [MODELS.GPT_4O_MINI]: { input: 0.15, output: 0.60 },
  [MODELS.DEEPSEEK_V3]: { input: 0.14, output: 0.28 },
}

// ============================================================
// Model Routing
// ============================================================

/** Task types that determine which model to use */
export type ModelTask =
  | 'bulk_sms'           // Gemini Flash - simple SMS personalization
  | 'standard_email'     // Gemini Flash - standard email personalization
  | 'quality_email'      // GPT-4o-mini - touch 2/3, missing data handling
  | 'preview'            // GPT-4o-mini - preview samples need consistent quality
  | 'edge_case'          // DeepSeek V3 - complex edge cases

/**
 * Route to appropriate model based on task type.
 * Returns AI SDK model instance ready for generateObject/generateText.
 *
 * Routing strategy from CONTEXT.md:
 * - Gemini Flash (70%): Bulk SMS, standard email, simple personalization
 * - GPT-4o-mini (25%): Quality personalization, touch 2/3, preview
 * - DeepSeek V3 (5%): Complex edge cases, experimentation
 */
export function getModelForTask(task: ModelTask) {
  switch (task) {
    case 'bulk_sms':
    case 'standard_email':
      return {
        model: google(MODELS.GEMINI_FLASH),
        modelId: MODELS.GEMINI_FLASH,
      }
    case 'quality_email':
    case 'preview':
      return {
        model: openai(MODELS.GPT_4O_MINI),
        modelId: MODELS.GPT_4O_MINI,
      }
    case 'edge_case':
      return {
        model: openrouter(MODELS.DEEPSEEK_V3),
        modelId: MODELS.DEEPSEEK_V3,
      }
  }
}

/**
 * Get a secondary/fallback model different from the primary.
 * Used when primary model fails validation (try different model before raw template).
 *
 * Fallback chain:
 * - Gemini Flash -> GPT-4o-mini
 * - GPT-4o-mini -> Gemini Flash
 * - DeepSeek V3 -> GPT-4o-mini
 */
export function getSecondaryModel(primaryModelId: ModelId) {
  switch (primaryModelId) {
    case MODELS.GEMINI_FLASH:
      return {
        model: openai(MODELS.GPT_4O_MINI),
        modelId: MODELS.GPT_4O_MINI,
      }
    case MODELS.GPT_4O_MINI:
      return {
        model: google(MODELS.GEMINI_FLASH),
        modelId: MODELS.GEMINI_FLASH,
      }
    case MODELS.DEEPSEEK_V3:
      return {
        model: openai(MODELS.GPT_4O_MINI),
        modelId: MODELS.GPT_4O_MINI,
      }
  }
}

/**
 * Determine task type from personalization context.
 * Used by personalizeMessage() to auto-route.
 */
export function inferModelTask(channel: 'email' | 'sms', touchNumber: 1 | 2 | 3 | 4): ModelTask {
  if (channel === 'sms') {
    return 'bulk_sms'
  }

  // Email: Touch 1 is standard, Touch 2-4 need quality variation
  if (touchNumber >= 2) {
    return 'quality_email'
  }

  return 'standard_email'
}

// Legacy exports for backward compatibility (existing code may use these)
export const DEFAULT_MODEL = MODELS.GPT_4O_MINI

export function getModel(modelId: string = DEFAULT_MODEL) {
  return openai(modelId)
}

export { openai }
```

Key design decisions:
- `getModelForTask()` returns both the AI SDK model instance AND the modelId string (for logging/cost tracking)
- `getSecondaryModel()` provides cross-model fallback (Gemini -> GPT-4o-mini and vice versa)
- `inferModelTask()` auto-determines task type from channel + touch number
- `MODEL_COSTS` exported for use by cost tracking in Plan 25-11
- Legacy `getModel()` and `DEFAULT_MODEL` kept for backward compatibility
- DeepSeek V3 accessed via OpenRouter's OpenAI-compatible endpoint (no extra package)
  </action>
  <verify>
    Run `pnpm typecheck` to confirm the module compiles. Verify @ai-sdk/google is in package.json dependencies.
  </verify>
  <done>
    - Three providers initialized (OpenAI, Google, OpenRouter)
    - getModelForTask() routes to correct model for each task type
    - getSecondaryModel() provides cross-model fallback
    - inferModelTask() auto-routes based on channel and touch number
    - MODEL_COSTS exported with per-model pricing
    - Legacy exports maintained for backward compatibility
    - TypeScript compiles
  </done>
</task>

<task type="auto">
  <name>Task 2: Update personalize.ts and fallback.ts to use multi-model routing</name>
  <files>
    lib/ai/personalize.ts
    lib/ai/fallback.ts
  </files>
  <action>
**1. Update `lib/ai/personalize.ts` to use routed model:**

Replace the `getModel` and `DEFAULT_MODEL` imports with `getModelForTask`, `inferModelTask`, and `ModelTask`:

```typescript
import { getModelForTask, inferModelTask, type ModelTask } from './client'
```

Modify the `personalizeMessage` function to:
- Accept an optional `task` parameter (defaults to auto-inferred from context)
- Accept an optional `modelOverride` parameter (for fallback secondary model usage)
- Use `getModelForTask(task)` instead of `getModel()`
- Return the actual `modelId` used (not hardcoded `DEFAULT_MODEL`)

Updated function signature:
```typescript
export async function personalizeMessage(
  ctx: PersonalizationContext,
  options?: {
    task?: ModelTask
    modelOverride?: { model: ReturnType<typeof getModelForTask>['model']; modelId: string }
  }
): Promise<PersonalizeResult> {
```

Inside the function:
- Determine which model to use:
  ```typescript
  const task = options?.task ?? inferModelTask(ctx.channel, ctx.touchNumber)
  const { model, modelId } = options?.modelOverride ?? getModelForTask(task)
  ```
- Replace `model: getModel()` in the `generateObject` call with `model`
- Replace `model: DEFAULT_MODEL` in the return with `model: modelId`

**2. Update `lib/ai/fallback.ts` to try secondary model on validation failures:**

Import `getSecondaryModel`, `inferModelTask`, and `getModelForTask` from client.ts.

Modify `personalizeWithFallback` to add a secondary model attempt when the primary fails with a validation error:

After the existing `backOff` try/catch block that catches the primary model call, add:

```typescript
// If primary model failed due to validation, try secondary model
// before falling back to raw template
if (errorMessage.includes('validation')) {
  try {
    const task = inferModelTask(ctx.channel, (ctx as PersonalizationContext).touchNumber)
    const primary = getModelForTask(task)
    const secondary = getSecondaryModel(primary.modelId)

    console.log(`Primary model validation failed, trying secondary: ${secondary.modelId}`)

    const secondaryResult = await withTimeout(
      personalizeMessage(ctx, { modelOverride: secondary }),
      LLM_TIMEOUT_MS
    )

    return secondaryResult
  } catch (secondaryError) {
    console.warn('Secondary model also failed, falling back to template:', secondaryError)
    // Fall through to raw template fallback below
  }
}
```

Place this BEFORE the final `return { message: ctx.template, ... }` fallback.

Important constraints:
- Secondary model attempt gets the same timeout (3s)
- No retry/backoff on secondary (single attempt only)
- If secondary also fails, fall through to raw template (never blocks sends)
- Log which models were attempted for debugging
- Do NOT change the `personalizePreviewBatch` function - it already uses `personalizeWithFallback` which will inherit the secondary model behavior

Update the `PersonalizeWithFallbackResult` type to optionally include `secondaryModel`:
```typescript
export type PersonalizeWithFallbackResult = {
  message: string
  subject?: string
  personalized: boolean
  fallbackReason?: FallbackReason
  model?: string
}
```
The `model` field already exists and will contain whichever model produced the final result (primary or secondary).
  </action>
  <verify>
    Run `pnpm typecheck` to confirm all files compile. Run `pnpm lint` to confirm no lint errors. Grep for `getModelForTask` in personalize.ts and `getSecondaryModel` in fallback.ts to confirm wiring.
  </verify>
  <done>
    - personalizeMessage() uses routed model via getModelForTask(inferModelTask(channel, touchNumber))
    - personalizeMessage() accepts optional model override for secondary model usage
    - fallback.ts tries secondary model on validation failures before falling back to template
    - Fallback chain: primary model -> secondary model -> raw template
    - Return value includes which model was actually used
    - personalizePreviewBatch inherits multi-model behavior
    - TypeScript and lint pass
  </done>
</task>

</tasks>

<verification>
1. `pnpm typecheck` passes
2. `pnpm lint` passes
3. `@ai-sdk/google` in package.json dependencies
4. client.ts exports getModelForTask, getSecondaryModel, inferModelTask, MODEL_COSTS
5. personalize.ts uses getModelForTask instead of hardcoded getModel()
6. fallback.ts attempts secondary model on validation failure before template fallback
7. No hardcoded `DEFAULT_MODEL` in personalize.ts return values
</verification>

<success_criteria>
- Three model providers configured (Google, OpenAI, OpenRouter)
- Model routing works: SMS -> Gemini Flash, Email Touch 1 -> Gemini Flash, Email Touch 2+ -> GPT-4o-mini
- Validation failures trigger secondary model attempt before raw template fallback
- Cost constants exported for downstream cost tracking
- TypeScript compiles, lint passes
</success_criteria>

<output>
After completion, create `.planning/phases/25-llm-personalization/25-10-SUMMARY.md`
</output>
