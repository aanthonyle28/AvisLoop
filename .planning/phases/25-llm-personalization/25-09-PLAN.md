---
phase: 25-llm-personalization
plan: 09
type: execute
wave: 1
depends_on: []
files_modified:
  - lib/ai/validation.ts
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "LLM output containing profanity triggers auto-fallback to raw template"
    - "LLM output containing inappropriate content (sexual, violent, discriminatory) triggers auto-fallback"
    - "Profanity detection stays within <50ms validation budget"
  artifacts:
    - path: "lib/ai/validation.ts"
      provides: "Profanity and inappropriate content detection patterns"
      contains: "profanity"
  key_links:
    - from: "lib/ai/validation.ts validateOutput()"
      to: "PROHIBITED_PATTERNS"
      via: "regex pattern matching"
      pattern: "contains_profanity"
---

<objective>
Add profanity and inappropriate content detection to LLM output validation. Currently, `validateOutput()` checks for incentives, urgency, pressure, and false claims via 14 regex patterns, but has zero profanity or inappropriate content detection. This is required by Success Criteria #9 (auto-fallback triggers include profanity/inappropriate content).

Purpose: Close gap from Phase 25 verification. Profanity in LLM output should trigger `contains_prohibited_content` validation failure, causing fallback to raw template.

Output: Updated validation.ts with profanity patterns added to PROHIBITED_PATTERNS and a new `contains_profanity` failure reason.
</objective>

<execution_context>
@C:\Users\aanth\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\aanth\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@lib/ai/validation.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add profanity and inappropriate content patterns to validation</name>
  <files>lib/ai/validation.ts</files>
  <action>
Three changes to `lib/ai/validation.ts`:

**1. Add `contains_profanity` to ValidationFailureReason union type:**

```typescript
export type ValidationFailureReason =
  | 'missing_review_link'
  | 'missing_opt_out'
  | 'missing_business_name'
  | 'too_long'
  | 'contains_html'
  | 'contains_unknown_url'
  | 'contains_prohibited_content'
  | 'contains_profanity'
  | 'contains_unresolved_token'
```

**2. Add profanity/inappropriate content patterns as a separate array** (keeps concerns clean):

Create a `PROFANITY_PATTERNS` array with regex patterns covering:

```typescript
// Profanity and inappropriate content patterns (SC #9 compliance)
// Uses word boundary matching (\b) to avoid false positives on substrings
const PROFANITY_PATTERNS: RegExp[] = [
  // Common profanity (word boundaries to prevent substring matches)
  /\b(?:fuck|shit|damn|ass|bitch|bastard|crap|dick|piss|cock|cunt|whore|slut)\b/i,
  /\b(?:wtf|stfu|lmao|lmfao)\b/i,
  /\b(?:goddamn|motherfuck|bullshit|horseshit|dipshit|dumbass|jackass|asshole|arsehole)\b/i,

  // Variants with character substitution (l33t speak)
  /\b(?:f[u*@]ck|sh[i!1]t|b[i!1]tch|a[s$]{2})\b/i,

  // Sexual content
  /\b(?:sex(?:ual|y)|porn|nude|naked|erotic|orgasm|genital)\b/i,

  // Violence
  /\b(?:kill\s+(?:you|him|her|them)|murder|assault|rape|stab|shoot)\b/i,

  // Discriminatory language
  /\b(?:racist|sexist|homophob|transphob|bigot|supremac)\b/i,
  /\b(?:fag(?:got)?|dyke|tranny|retard(?:ed)?|spic|chink|kike|nigger|wetback)\b/i,

  // Threats
  /\b(?:threat(?:en)?|i'?ll\s+(?:hurt|harm|destroy|ruin))\b/i,

  // Drug references (inappropriate for business messages)
  /\b(?:cocaine|heroin|meth(?:amphetamine)?|marijuana|weed|drugs?)\b/i,
]
```

Important notes on the patterns:
- Use `\b` word boundaries to prevent false positives (e.g., "class" matching "ass", "Scunthorpe" matching a slur)
- Keep the list focused on content that would be CLEARLY inappropriate in a business review request message
- This is not an exhaustive list - it covers the most common cases to catch LLM failures
- All patterns are case-insensitive (`/i` flag)
- Must stay within <50ms validation budget (regex with word boundaries is fast)

**3. Add profanity check to `validateOutput()` function:**

After the existing `// 6. No prohibited content` check (around line 172) and before the `// 7. No unresolved template tokens` check, add:

```typescript
  // 7. No profanity or inappropriate content (LLM-08 requirement)
  for (const pattern of PROFANITY_PATTERNS) {
    if (pattern.test(output)) {
      return { valid: false, reason: 'contains_profanity' }
    }
  }
```

Then renumber the existing unresolved token check to `// 8.`

This keeps profanity detection as a separate concern from prohibited business content (incentives/urgency/pressure) while using the same fail-fast pattern.

Do NOT change the `\b(?:...)` patterns to remove the word boundary anchors - they prevent the "Scunthorpe problem" (false positives from legitimate words containing profane substrings).

Do NOT add more than ~10 regex patterns to keep within the <50ms budget. The patterns above cover the major categories with combined alternation groups.
  </action>
  <verify>
    Run `pnpm typecheck` to confirm the new type compiles. Run `pnpm lint` to confirm no lint errors. Manually verify the patterns array exists and the validateOutput function includes the profanity check step.
  </verify>
  <done>
    - `contains_profanity` is a valid ValidationFailureReason
    - PROFANITY_PATTERNS array has patterns covering profanity, sexual, violent, discriminatory, threat, and drug content
    - validateOutput() checks profanity patterns after prohibited content check
    - Word boundary matching prevents false positives (e.g., "class" does not match)
    - Validation stays within <50ms budget (regex with alternation groups is fast)
    - TypeScript and lint pass
  </done>
</task>

</tasks>

<verification>
1. `pnpm typecheck` passes
2. `pnpm lint` passes
3. PROFANITY_PATTERNS array exists in validation.ts with at least 8 regex patterns
4. validateOutput() includes profanity check step that returns `contains_profanity` reason
5. Word boundaries (\b) used to prevent false positive matches
</verification>

<success_criteria>
- LLM output containing profanity would fail validation with reason `contains_profanity`
- Inappropriate content (sexual, violent, discriminatory) also detected
- No false positives on common business words (class, assessment, analytics, etc.)
- Validation performance stays within <50ms budget
- TypeScript compiles, lint passes
</success_criteria>

<output>
After completion, create `.planning/phases/25-llm-personalization/25-09-SUMMARY.md`
</output>
